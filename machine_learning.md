# machine_learning
记录复习笔记和相关项目

## 传统机器学习
### 一些基本知识
机器学习三要素：模型、策略（最小模型误差）、算法（优化算法）

机器学习一般可以分为有监督（回归和分类）、无监督（聚类和降维）和强化学习

特化与泛化、归纳偏好、奥卡姆剃刀

NFL定理：在所有可能的机器学习任务（问题）上，任意两个模型的‘平均性能’是完全相等的 —— 没有任何一个模型能在所有任务上都比另一个模型表现更好

过拟合  
* 原因：噪声、数据有偏
* 解决办法：
  * 数据增强（有监督、无监督）
  * 降低模型复杂度：正则化（Lasso，岭回归）、减少特征选择（dropout，早停，集成学习）

数据集的划分：留出法、交叉验证、自助法

性能指标
* 分类
  * E（错误率）= a/m  Accuracy = 1-E
  * 查准率(P) = $\frac{TP}{TP+FP}$
  * 查全率(R) = $\frac{TP}{TP+FN}$
  * F1 = $\frac{2PR}{P+R}$
* 重要曲线
  * PR曲线： 纵轴是P(查准率)，横轴是R(查全率)
  * ROC曲线：    纵坐标是真正例率(正例中预测为真的比率)，横坐标是假正例率(所有假例中被预测为正的比率)，AUC是线下面积
  * ks曲线：    按照预测值降序排列，ks值 = max(正样本累计占比-负样本累计占比)     衡量区分正负样本的能力。此时阈值为最合适的阈值

### 特征工程

完整的特征工程包括对数据预处理，特征提取，特征转换（降维升维），和预测识别得到结果。

> 降维手段包括特征提取和特征选择，特征提取包括PCA和LDA（线性判别分析），特征选择包括互信息和TF-IDF

对特征的处理是机器学习（传统）中最重要的

分类特征可采用：映射编码（代数量不等），二进制编码（便于运算），独热编码（最优）

### 模型

1. 线性模型： 最小二乘法，R^2 , R^2-adjusted

2. 逻辑回归： 将线性模型包装为概率，包装函数为**Sigmoid**函数：
   $$
   σ(z)=\frac{1}{1+e^ {-z}}
   $$

3. 决策树

   - 建树依据：1. 基尼系数 $ gini = 1-\sum{p^2}$ (越小越好)   2.     信息增益：   信息熵的差值  ， 信息熵：$ H(x) = -\sum{p \log_{2}(p)} $ 
   - 剪枝：前剪枝，后剪枝

4. 朴素贝叶斯分类器：假定特征独立，用贝叶斯公式。可以分为高斯分类器（特征连续）和多项式分类器（特征独立），下面是多项式分类器：
   $$
   P(y=1 \mid x) \propto P(y=1) \cdot \prod_{i} P(x_i \mid y=1)
   $$

   $$
   P(y=0 \mid x) \propto P(y=0) \cdot \prod_{i} P(x_i \mid y=0)
   $$

   比较两者大小

   优点：速度快，容易解释，计算快，很适合处理高维数据

   缺点：数据分布很难符合（连续），需要区分度很高

   拉普拉斯平滑

5. 支持向量机：是一种判别分类方法，不同于朴素贝叶斯生成分类方法。是人为分割的，而不是本就存在这样的分类。用一个平面去分类，使得两边间隔最大。如果线性不可分，考虑软分割或者核变换。

6. 主成分分析（无监督）：

   - 主成分 ： 找到数据中方差最大的若干相互垂直的方向（最能表示差异）具体步骤分为数据标准化、计算协方差矩阵，求特征值和特征向量，选择前k个主成分（用累计贡献率来判断k的值，大约八九成即可）建立坐标系，将数组投影到新坐标系

7. K近邻 ：有监督分类方法，用近k个邻居进行投票。要先标准化

8. K均值：无监督聚类方法。随机找k个作为中心点，距离近的归为一类，重新计算中间点，无线迭代。

   ​              聚类效果依赖k的取值和初次选点。调参方法：网格搜索（遍历）

9. DBSCAN: 设定固定长度，所有距离在固定长度内的点被归为一类

10. 集成学习

    - bagging (随机森林): 随机抽取一部分特征来建立树，最后多个数来进行投票
    - boosting:
      - Adaboost: 每轮都用弱学习器来训练，前一轮被错误分类的样本会被调大权重，使得后面的学习器更关注错误分类的样本。最后根据错误率来调整各个学习器的权重并加以组合，得到强学习器。
      - GBDT(梯度提升决策树): 核心思想是通过**迭代训练多个决策树（弱分类器），每个新树都去拟合前一轮模型的预测残差**，最终将所有树的预测结果相加得到最终输出。
      - XGBoost: 升级梯度提升决策树。在损失函数加入正则化，算梯度的时候用二阶泰勒（不是GBDT的一阶），同时分裂时选择最优点
      - lightGBM: 升级梯度提升决策树 。每次只分裂 “增益最大的叶子节点”，直方图算法，并行算法

### 代码











